---
title: "Style GAN 개념 콕콕?"
date: 2020-08-25
categories: StyleGAN
---

https://comlini8-8.tistory.com/11 본글은 이 블로그를 토대로 작성되었습니다. 감사합니다 !

GAN의 주요 과제중 한가지는 포즈,얼굴형,머리 스타일 과 같은 구체적인 특징을 바꿀 수 있도록 아웃풋을 컨트롤 하는것이다.
NVDIA에서 발표한 새로운 논문, 'A Style-Based Generator Architecture for GANs(StyleGAN)'은 이 과제를 해결하는 새로운 모델을 제시한다. StyleGAN은 매우 낮은 화질에서 시작해 높은 화질(1024x1024)로, 인위적인 이미지를 점진적으로 생성합니다. 각 단계(화질)에서 인풋을 수정함으로써 포즈나 얼굴형과 같은 굵직한 특징부터 머리색과 같은 디테일까지 해당 단계에서 표현되는 특징을 조절한다. 이때, 다른 단계에 영향을 주지 않는다

* StyleGAN의 baseline이 되는 모델은 PGGAN이다 PGGAN의 가장 큰 특징은 저화질에서 시작해 점점 레이어를 쌓아가며, 고화질 이미지를 생성한다는 점이다.

![pggan](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FHzgm0%2FbtqCHv30nyF%2F8pmtVtU1vfKqG9bHrX2bT1%2Fimg.png)

이러한 기술은 생성되는 아웃풋에 대해 더 잘 이해할 수 있게 할 뿐 아니라 , 또한 이전에 생성된 이미지들보다 더 진짜 같이 보이는 고해상도의 이미지를 만들어 낸다.

![styleganex](https://hichoe95.tistory.com/91)

### Background

모든 Gna의 기본요소는 두개의 뉴럴 넷이다 Generator는 새로운 샘플을 합성하는 모델 Discriminator는 학습데이터와 G의 아웃풋을 인풋으로 구별을 한다 .
proGAN의 핵심아이디어는 progressive training이다 Progrssive triaing은 G와 D를 아주 낮은 화질의 이미지로부터 학습하며 시작한다. 그리고 높은 화질의 레이어를 추가해간다.

이 기술은 낮은 화질의 이미지에도 나타나는 기본적인(base)를 학습해 이미지의 토대를 먼저 만들고ㅡ 화질을 점점 높여가며 디테일을 학습한다. 저화질의
이미지를 학습하는 것은 쉽고 빠를뿐 아니라 고화질의 사진 학습을 돕고 결과적으로 전체적인 학습또한 빨라진다.

![progan](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdKM169%2FbtqCJlmgTUW%2FbjKtqshu7r1Fk1ORiny5s1%2Fimg.png)

ProGAN은 고품질 이미지를 생성하지만, 대부분의 모델과 같이 생성된 이미지의 구체적인 특징을 컨트롤하는 능력은 매우 제한적이다, 다시 말해 특징들이 얽혀있고
따라서 인풋을 조금이라도 조정하게 된다면 동시에 여러 특징에 영향을 미치게 된다 하나의 유전자를 바꾸어도 여러 특성에 영향을 주는 상황이 좋은 비유가 될것이다.

### How StyleGAN works

StyleGAN 논문은 Generator네트워크에 초점을 맞추어, proGAN의 이미지 generator의 업그레이드된 버전을 제안한다, 저자는 ProGAN의 progressive 
레이어를 적절히 활용할 경우, 이미지의 다양한 시각적 특징을 컨트롤할 수 있음을 알아냈다. 레이어단계가 낮을수록, 논문은 특징을 세 가지 종류로 분류했다.

1. Coarse(굵직한 특성) -8 해상도까지 (4X4 layer ~ 8x8 layer) -포즈, 일반적인 헤어스타일, 얼굴형 등에 영향

2. Middle(중간특징)- 16부터 32해상도까지 - 자세한 얼굴 특징, 헤어스타일, 눈뜨고/감음 등에 영향

3. Fine(자세한 특징)- 64부터 1024 해상도까지 -눈, 머리 ,피부등의 색 조합과 미세한 특징등에 영향 

새로운 G(stylegan)는 ProGAN의 G에 몇가지 추가사항을 포함한다.

### Mapping Network 

매핑 네트워크의 목표는 인풋 벡터를 각기 다른 시각적 특징을 다른 요소로 컨트롤 할 수 있는 중간(intermediate)벡터로 인코딩 하는 것이다.
학습데이터의 확률 밀도를 따라야 하기 때문에, 인풋 벡터로 시각적 특징을 컨트롤 하는것은 제한적이다.
https://comlini8-8.tistory.com/10 - 확률분포 관련된 내용 

기존의 GAN 의 Generative 모델의 문제점은 아래 그림과 같이 특징들이 서로 얽혀있어서 벡털르 조절하면 얽힌 여러 특징이 동시에 변하게 된다는 것이다. StyleGAN논문에서는 이러한 문제를 'entanglement'라고 부른다, Mapping metwork는 StyleGAN팀에서 제안한 entanglement 문젤르 해결하는 방법이다. 

만약 데이터셋에 검은 머리의 사람이 많을 경우,  더 많은 인풋값들이 그러한 특성에 매핑되게 된다. 결과적으로 모델은 일부 인풋의 일부분을 특징과 제대로 매핑하지 못하게된다. 이러한 현상을 우리는 feature entanglement라고 부른다. 하지만 또 다른 뉴럴 네트워크를 이용함으로써 모델은 학습 데이터의 분포를 따를 필요가 없는 벡털르 만들게 되고 특징 간의 상관관계를 줄일 수 있게된다.

매핑 네트워크는 8개의 fully connected 레이어로 구성되어있고, 아웃풋 w는 인풋 레이버(512x1)와 같은 사이즈가 된다. 이 과정을 거치면 input vector(z)로 부터 이미지를 생성하는 것이 아닌 intermediate vector(w)로 먼저 변환한 후 이미지를 생성한다는 것이다.

![the generator](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F31quP%2FbtqCF6DWtmy%2FoGkAoQ0dlfKzkiU50Sgou0%2Fimg.png)

### Style Modules(AdaIN) 합성 네트워크(Synthesis Network)

![네트워크](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FlKHPU%2FbtqDEG4wB0b%2FttCTi5kEQkMEODFPJf2FK1%2Fimg.png)

AdaIN(Adaptive Instance Normalization) 모듈은 매핑 네트워크에서 생성되느 인코딩 된 정보 W를 생성된 이미지로 전달 (transfer)한다.이 모듈은 합성 네트워크(Synthesis Network)의 각각의 단계에 더해지고 해당 단계에서의 시각적인 표현을 정의한다. 

Style의 흐름을 전체적으로 간단히 설명하자면, Mapping Network f에 있어서, 입력되는 대체 변수 z를 대체공간 W에 맵핑한다. 대체변수 w는 아핀 변환되는 것으로 Style y가 된다. 스타일 y는 각 Convolution층을 거친 후의 AdaIn처리에 사용되고, Generator을 컨트롤한다. 가우시안 노이즈를 AdaIn과 각 Convolution층의 사이에 더 한다. 마지막층의 출력은 1x1의 Convolution층을 사용해 RGB로 변환한다. 해상도는 4x4에서부터 시작하여 최종적으로는 1024x1024가 된다.

### AdaLN 이란?

AdaLN이란 스타일 변환 활용의 정규화 방법으로 수식은 아래와 같다, 원래의 논문에서는 콘텐트 입력 x와 스타일 입력 y를 평균과 분산을 이용해 정규화한다 .

![수식](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbfr4lF%2FbtqDDYqMxv4%2Flijv24fhhtMYLo3yMWxqOk%2Fimg.png)

![aladin](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdVCUeb%2FbtqDE0PatfJ%2F7rLt6PHd9upFko813bgPIk%2Fimg.png)

 AdaIn의 특징은 instance Normalization등의 정규화 방법과 달리, 스타일과 콘텐츠 이미지의 총합량만 정규화하고, 학습 파라미터를 사용하지 않는다는 점이다, 이에 따라 훈련 데이터에서 본적 없는 스타일이라도 스타일 변환이 가능해졌다.
 
 StyleGAN중에서 AdaIn은 아래의 수식을 사용한다. 정규화된 콘텐츠 정보에 스타일을 사용한 선형변환을 적용한다는 개념은 변화하지 않고 있지만, 스타일의 표준편차와 평균치 대신 뒤에서 설명하는 스타일 벡터 W에 선형 변환을 더한 y_s, y_b라는 2개의 값을 사용한다.
 
 ![수식2](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbLWclD%2FbtqDEZbFbDS%2FNSPb8l5s1rUp6EBlVXFdQ1%2Fimg.png)
 
 위의 그림을 보자 양쪽 모두 해상도를 상승시키는 Progressive Growing을 사용하고 있다. 그러나 PG-GAN 에서는 자맺변수 z에서부터 이미지를 생성하고 있는 것에 반해, StyleGAN에서는 고정의 4x4x512의 Tensor로부터 이미지를 생성하고 있으며, 잠재변수는 Style로써 거두어 들이고 있다는 점에서 다른점이있다. 또한 잠재변수는 그대로 사용되지 않고 Mapping network라고 불리는 FCN에 의해 비선형변환된 후에 Style 로 사용된다.
 
 A: w를 style( y_s , y_b y_s , y_b)로 바꾸기 위한 아핀변환 ys,ybys,yb는 채널마다 값을 갖는다.- s:scale b:bias
 중간벡터 w는 A로 표시된 또 다른(fully connected)레이어를 거쳐 각 
 
 B: 가우시안 노이즈는 1채널 이미지로 이루어짐 conv의 출력에 더하기 전에 노이즈를 채널마다 scaling하는 것을 의미 
 
 이러한 
 
 
 
 






