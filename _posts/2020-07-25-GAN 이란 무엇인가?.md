---
title: "What is GAN?"
date: 2020-07-25 18:37:00 -0400
categories: GAN
---


## GAN이란? 

**GAN은 ‘Generative Adversarial Network’의 약자다. 이 세글자의 뜻을 풀어보는 것으로 GAN에 대한 전반적 이해를 할 수 있다.**

첫단어인 'Generate'은 GAN이 생성(Ganeration) 모델이라는 것을 뜻한다 생성모델이란 '사실을 기반으로는 정교한 가짜' 라고 할 수 있다.  
즉 현실세계에 존재하지 않지만 현실세계에 존재 할 것만 같은 모델을 생성해 낸다는 것을 의미한다. 

![ganexam](http://sanghyukchun.github.io/images/post/92-3.png)

그렇다면 있을 법하다는 것을 어떻게 정의를 해야할까? 수학적 실제데이터의 분포와 만들어낸 데이터의 분포를 비교하여 비슷하면 가능하다.  
예를들어 귀여운 고양이를 떠올려 보자 그리고 고양이를 정의 하는 데이터를 몸통 길이와 다리길이라 가정해보자 고양이의 종류는 나는 잘모르지만 일단 적지는 않을 것이다 하지만 몸통길이가 1M 다리길이가 1M 씩하는 고양이는 현실세계에서는 듣도 보지도 못하였을 것이다 이러한 조합은 실제 데이터 분포에서는 거의 나오지 않는 조합이기 떄문이다 따라서 우리가 만들고자 하는 것은 수학적으로 생성모델을 만들어 이 모델이 실제 데이터 분포와 근사하게 만드는 것이 목표로 삼아야 한다.

이 세상에 고양이를 몸통길이와 다리길이로 모든 특징을 구현할 수 있으면 좋겠지만 현실은 그렇지 않다 만약 이미지 파일의 크기가 256x256픽셀인 경우 RGB컬러 값까지 총 256x256x3의 경우를 모두 고려해야 한다. 그 변수들의 조합은 우연하게 고양이 처럼 보이는 픽셀 값을 가지게 된다면 문제 가 없지만 이는 상당히 희망적인 관측론이고 실제로는 매우 어려운 문제이다. 하지만 발전하는 딥러닝 기술은 이를 가능캐 하였다.

GAN의 두번째 단어인 A는 'Adversarial' 의 약자이다. 이는 GAN이 두 개의 모델을 적대적(Adversarial)적으로 경쟁시키며 발전시킨다는 의미이다. 뜬금없지만 위조지폐범과 경창을 생각해보자. 이둘은 적대적인 관계이다 위조지폐범이 경찰을 속이기 위하여 위조지폐기술은 더욱 발전할 것이고 이를 판별하기 위해 경찰의 판별기술 또한 발전할 것이다. 이윽고 시간이 흐르면 위조지폐범의 위폐 제조 기술은 완벽에 가까울 것이다.

위의 예처럼 GAN은 위조지폐범에 해당하는 생성자(generrator) 역할 그리고 경창에 해당하는 구분자(discvriminer)의 역할을 가지게 된다. 생성자의 목적은 진짜와 같은 가짜데이터를 생성하여 구분자를 속이는 것이며 구분자의 목적은 생성자가 만든 가짜 데이터를 구분하는 것이다. 이둘을 함께 학습 시키면서 진짜와 구분할 수 없는 가짜를 만들어 내는 생성자를 얻을 수 있다. 이것이 바로 GAN의 핵심아이디어인 적대자 학습(Adverarial Learning)이다.

![gan](https://files.slack.com/files-pri/T25783BPY-F9SHTP6F9/picture2.png?pub_secret=6821873e68)

GAN의 마지막 단어인 N은 "Network' 이다. 이는 이 모델이 인공신경망(Artifical Nerual Network) 혹은 딥러닝(Depp Leraning)으로 만들어 졌기 때문에 포함되었다. 하지만 적대적 학습을 위하며 무조건 딥러닝을 써야 하는건 아니다 하지만 딥러닝 구조의 강력한 모델링은 강력한 머신러닝 구조를 만들 수 있게 해준다 딥러닝이 어러한 힘을 얻게 비결은 비선형 활성함수(non-Linear Activation Function)와 계층(Hierarchy)구조, 그리고 역전파(Backpropagation)으로 꼽을 수 있다.

## GAN의 작동 원리

GAN 의 동작 단게는 다음과 같다.

* generator가 임의의 수를 입력받아 생성한 이미지로 반환한다.

* 이렇게 생성된 이미지는 실제 데이터 세트에서 가져온 이미지들과 함께 discriminator에 전달된다.

* discriminator은 실제 이미지와 가짜 이미지를 판별하여 0과 1사이의 확률값으로 반호나한다 1은 실제이미지, 0은 가짜 이미지를 의미한다

따라서 이중 피드백 루프를 갖게 된다.

* discriminator는 이미지의 정답 값으로 부터 피드백을 받는다.

* generator는 discriminator로 부터 피드백을 받는다.

![gnas](https://pathmind.com/images/wiki/gan_schema.png)

discriminator 네트워크가 전달된 이미지를 실제 이미지인지 가짜 이미지인지를 판별할 수 있는 일반적인 컨볼루션 네트워크라면 generator는 정반대의 컨볼루션 네트워크이다. 일반 컨볼루션 분류기는 이미지를 입력받아 확률을 예측하기 위해 이를 다운 샘플링하는 반면 generator는 랜덤 노이즈 벡터를 입력받아 이미지를 만드는 업샘플링을 한다, 즉 일반적인 컨볼루션 네트워크는 maxpooling 과 같은 다운 샘플링 기술을 사용하여 데이터를 처리하고 generator와 같은 inverse 컨볼루션 네트워크는 새로운 데이터를 생성한다.

```
업 샘플링은 해당 분류에 속하는 데이터가 적은 쪽을 표본으로 더 많이 추출하는 방법이며,
다운 샘플링은 데이터가 많은 쪽을 적게 추출하는 방법이다.
```
두 네트워크 모두 제로섬 게임처럼 서로 반대되는 목적함수 또는 손실함수를 통해 최적화하려고 시도한다 이것은 actor-critic model과 비슷하다. discriminator의 행동이 바뀌면 generator의 행동도 변하고 그 반대의 경우도 마찬가지이다. 각 에러는 서로에게 전달된다.

![gannetwork](https://pathmind.com/images/wiki/GANs.png)

## GAN의 전성시대를 연 DCGAN(Deep Convolution GAN)

GAN은 학습이 불안적하기로 악명이 높다 GAN의 두 개의 네트워크를 트레이닝 하다 보면 다음과 같은 문제가 생길 수 있다. discriminator가 너무 뛰어나면 0이나 1에 매우 가까운 gradient값을 반환하게 되어, generator가 gradient값을 제대로 반영하기 어렵게 된다. generator가 너무 뛰어나면 discriminator가 진짜 데이터를 가짜 데이터로 판단할 확률이 높아진다. 이러한 문제는 두 신경망의 학습률(learning rates)을 각각 설정하여 완화할 수 있다. 두 개의 신경망은 항상 비슷한 “학습 수준” 1을 유지해야 한다.

이러한 학습이 어렵다는 점은 GAN모델이 다양한 곳에서 응용되는 것을 가로막는 큰 장애물이었다. 이러한 상황에서 수많은 실험끝에 안정적인 학습이 가능한 GAN모델의 구조를 찾아낸 것이 DCGAN이다.

DCGAN의 특징은 몇가지로 요약할 수 있는데 우선 선형레이어(Linear Layer)와 풀링레이어(Pooling Layer)를 최대한 배제하고 합성곱(Convolution)과 'Transposed Convolution(Fractional-Strided Convolution)으로 네트워크 구조를 만들었다. 풀링레이어는 여러 딥러닝 모델에서 불필요한 매개변수의 수를 줄이고 데이터에서 중요한 특징만을 골라내는 장점이 있다. 하지만 이미지 데이터인 경우 그 위치정보가 잃어버리게 된다는 단점이 있다. 이미지 생성에 있어 위치정보는 매우 중요한 요소이기 떄문에 DCGAN은 풀링레이어를 배제하였다. 선형 레이어 또한 마찬가지로 위치정보를 잃어버리므로 사용하지 않았다. 

![dcgan](https://files.slack.com/files-pri/T25783BPY-F9SHY37JT/picture7.png?pub_secret=a4ad9b1733)

DCGAN의 또다른 특징은 배치 정규화(Batch Nomalization)를 사용했다는 점이다. 배치 정규화는 레이어의 입력 데이터 분포가 한 방면으로 치중되어있을 경우 평균과 분산을 조정해준느 역할을 한다 이는 역전파가 각 레이어에 쉽게 전달되도록하여 학습이 안정적으로 이뤄지도록 돕는다.

이외에도 DCGAN은 수많은 실험을 통해 GAN을 학습시키는 가장 좋은 조건들을 찾아냈다. DCGAN은 마지막 레이어를 제외하고 생성자의 모든 레이어에 ReLU를 사용했고, 구분자의 모든 레이어에 LeakyReLU를 사용했다. 또한, 가장 좋은 최적화 기법과 적절한 학습 속도(Learning Rate) 등을 찾아내기도 했다.

DCGAN의 성공은 GAN 모델이 유명해지는 데 결정적인 역할을 했다. DCGAN에서 사용한 모델 구조는 아직도 새로운 GAN 모델을 설계할 때 베이스 모델이 되고 있다.

DCGAN의 네트워크 구조는 기존 GAN에서 생성자와 구분자만 교체하는 것만으로 간단히 구현할 수 있다. DCGAN의 생성자는 GAN과 마찬가지로 랜덤 벡터 z를 받고 가짜 이미지를 생성하는 함수다. 다만 그 구현에서 ‘Transposed Convolution’과 배치 정규화 등을 사용한다는 점이 다르다.

------------------------------------------------

## 이미지를 새로운 이미지로 변형하는 cGAN

때때로 이미지를 처음부터 생성하기보다 이미 있는 이미지를 다른 영역의 이미지로 변형하고 싶은 경우가 많다. 예를 들어, 스케치에 채색하거나, 흑백 사진을 컬러로 만들거나, 낮 사진을 밤 사진으로 바꾸고 싶을 때 등이다. ‘cGAN(Conditional GAN)’은 이를 가능케 해주는 모델이다.

기존의 GAN의 생성자가 랜덤 벡터를 입력으로 받는 것에 비해 cGAN의 생성자는 변형할 이미지를 입력으로 받는다. 그 뒤 생성자는 입력 이미지에 맞는 변형된 이미지를 출력한다. 예를 들어 스케치 사진을 받은 생성자는 그 스케치에 맞는 색을 칠한 뒤 채색된 이미지를 출력하는 것이다. 구분자는 스케치와 채색된 이미지를 모두 보고 그 채색된 이미지가 과연 스케치에 어울리는지 판단한다. 구분자를 속이기 위해서 생성자는 첫째, 진짜 같은 이미지를 만들어야 하고 둘째, 스케치에 맞는 이미지를 만들어야 한다.

![CGAN](https://files.slack.com/files-pri/T25783BPY-F9SBYT06A/picture9.png?pub_secret=fa4da6cab2)

cGAN의 혁신은 주어진 이미지를 새로운 이미지로 변형하는 수많은 문제를 하나의 간단한 네트워크 구조로 모두 풀었다는 점이다. 모든 문제는 이미지에서 의미적인 정보를 찾아내어 다른 이미지로 바꾸는 문제로 볼 수 있기 때문이다. 이렇게 한 영역의 이미지를 다른 영역의 이미지로 변형하는 문제의 경우 cGAN이 유용하게 쓰일 수 있다.

------------------------------------------------------------------

## MNIST데이터 이미지 생성 (실습코드)


위와 같은 DCGAN구조는 이전의 GAN보다 훨씬 안정적이며 훌룡한 결과를 얻어낼 수 있다. 이러한 DCGAN으로 MNIST에제 이미지를 학습하여 손글씨 숫자 이미지를 생성해볼 것이다

아래 코드는 예제코드이며 아래코드를 바탕으로 공부하였다.

~~~
import numpy as np
from keras.models import *
from keras.layers import *
from keras.optimizers import  *
from keras.datasets import mnist

import keras.backend as K
import matplotlib.pyplot as plt

K.set_image_data_format('channels_last') 
~~~
특별한 것이 없는 코드지만 set_image_data_format 함수는 이후 합성곱 신경망에서의 형태를 설정해준다.

~~~
class Gan:
  def __init__(self, img_data):
    img_size = img_data.shape[1]#28
    channel = img_data.shape[3] if len(img_data.shape) >= 4 else 1 #이건 뭔지 모르게슴ㅎㅎ
  
    self.img_data = img_data
    self.input_shape = (img_size, img_size , channel)
    self.img_rows = img_size
    self.img_cols = img_size
    self.channel = channel
    self.noise_size = 100

    #creat D ang G
    self.create_d()
    self.create_g()
~~~
 mnist예제 코드의 데이터형태는 mnist.train.images[55000, 28, 28] 이므로 img_size는 28를 반환한다. 이후 클래스에 각 데이터를 정의해주고 noise 1차원 벡터의 값은 100으로 두었다.
 
 ~~~
     #Build model to train D.
    optimizer = Adam(lr = 0.0008) #이떄 lr은 학습률이다
    self.D.compile(loss = 'binary_crossentropy' , optimizer = optimizer)

    #Build model to train G.
    optimizer = Adam(lr = 0.0004)
    self.D.trainable = False
    self.AM = Sequential()
    self.AM.add(self.G)
    self.AM.add(self.D)
    self.AM.compile(loss='binary_crossentropy' , optimizer = optimizer)
~~~
Discriminer(식별자)의 학습방법을 정의 하였다  Optimizer 모델의 한종류인 Adam을 사용하며  
적대적 모델(Adversarial Model) AM은 아래와 같이 생성자 - 식별자가 함께 쌓아 놓은 것을 가지고 학습한다. 생성자 부분은 멍청한 식별자(처음 생성했을 때)와 함께 피드백으로 부터 학습하는 방식으로 한다. 

![dcgan](https://gluon.mxnet.io/_images/dcgan.png)

~~~
def create_d(self):
    self.D = Sequential()
    depth = 64
    dropout = 0.4
    self.D.add(Conv2D(depth*1 , 5, strides = 2, input_shape = self.imput_shape , padding = 'same'))
    self.D.add(LeakyReLU(alpha = 0.2))
    self.D.add(Dropout(dropout))
    self.D.add(Conv2D(depth*2,5,strides =2,padding='same'))
    self.D.add(LeakyReLU(alpha=0.2))
    self.D.add(Dropout(dropout))
    self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))
    self.D.add(LeakyReLU(alpha=0.2))
    self.D.add(Dropout(dropout))
    self.D.add(Conv2D(depth*8 , 5 , strides = 1 , padding = 'same'))
    self.D.add(LeakyReLU(alpha = 0.2))
    self.D.add(Dropout(dropout))
    self.D.add(Flatten())
    self.D.add(Dense(1))
    self.D.add(Activation('sigmoid'))
    self.D.summary()
    return self.D
 ~~~
 
 MNIST 데이터셋으로 28(pix)x28(pix)x1(channel)의 이미지를 입력 데이터로 한다.  
 시그모이드(sigmoid) 결과는 실제 이미지 정도의 확률을 단일 값으로 버여준다. 전형적인 CNN과 다른점은 레이어 사이에서 최대풀링(max-pooling)이 없다는 것이다. 대신에 다운 샘플링(downsampling), 임의로 이미지의 일부를 샘플링하는것) 을 사용한다.  
 각 CNN 레이어의 활성함수는 ReLU를 사용한다 레이어 사이에 0.4 ~ 0.7의 값을 가지도록 드랍아웃 값을 설정하면 과적합(over fitting)과 암기(특정 값을 외우는것), memorization을 방지할 수 있다.
 
 ~~~
  def create_g(self):
    self.G = Sequential()
    depth = 64+64+64+64 #256
    dropout = 0.4
    dim = 7

    self.G.add(Dense(dim*dim*depth, input_dim = self.noise_size))
    self.G.add(BatchNormalization(momentum = 0.9))
    self.G.add(Activation('relu'))
    self.G.add(Reshape((dim, dim, depth)))
    self.G.add(Dropout(dropout))
    self.G.add(UpSampling2D())
    self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))
    self.G.add(BatchNormalization(momentum=0.9))
    self.G.add(Activation('relu'))

    self.G.add(UpSampling2D())
    self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))
    self.G.add(BatchNormalization(momentum=0.9))
    self.G.add(Activation('relu'))

    self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))
    self.G.add(BatchNormalization(momentum=0.9))
    self.G.add(Activation('relu'))
    #28x28 x1 grayscale image
    self.G.add(Conv2DTranspose(1, 5, padding='same'))
    self.G.add(Activation('sigmoid'))
    self.G.summary()
    return self.G
~~~
생성자(Generator)는 가짜 이미지를 합성한다 -1.0 ~ 1.0 사이의 균등 분포를 가진 100차원의 노이즈를 가지고 컨볼루션의 역(inverse of convolution)으로 가짜 이미지를 생성한다.(이미지가 커지는 의미를 가지고 있다.) DCGAN에서 권장하는 단편적으로 향상되는 컨볼루션(fractionally-strided convolution) 대신에. 더 사실에 가까운 손글씨 이미지를 합성하기 위해 첫 새개의 레이어에서 업샘플링(upsampling)을 한다. 

레이어들 사이에 배치 표준화(batch normalization)는 학습하는데 안정성을 부여한다.
각 레이어 이후의 활성 함수(activation function)은 ReLU를 사용하고 최종 레이어에서 시그모이드(sigmoid)의 결과는 가짜 이미지를 생산한다.  
첫레이어에서 0.3 ~ 0.5의 값으로 드랍아웃(dropout)을 설정하면 과적합(overfitting)을 방지할 수 있다.

~~~

 def train(self , batch_size = 100):
    #pick image data randomly.
    images_train  = self.img_data[np.random.randint( 0, self.img_data.shape[0] , size=batch_size), :, :, :]
    #numpy.random.randint(low, high, (n, m), dtype=None)을 이용하여 low ~ high-1 사이의 무작위      (n, m) 크기정수 배열을 반환합니다.

    #Generate images from noise
    noise = np.random.uniform(-1.0, 1.0, size=[batch_size, self.noise_size])
    images_fake = self.G.predict(noise)

    #Train D.
    x= np.concatenate((images_train, images_fake))#배열을 합치는 함수이다
    y = np.ones([2*batch_size , 1])#두개의 이미지 데이터
    y[batch_size:, :] = 0
    self.D.trainable = True
    d_loss = self.D.train_on_batch(x,y)#하나의 데이터 배치에 대해서 경사 업데이트를 1회 실시

    #Train G
    y= np.ones([batch_size , 1])
    noise = np.random.uniform(-1.0 , 1.0 , size = [batch_size, self.noise_size])#랜덤실수뽑기
    self.D.trainable = False
    a_loss = self.AM.train_on_batch(noise , y)

    return d_loss, a_loss, images_fake
    
 ~~~
    
훈련은 가장 어려운 부분인데 우리는 식별자모델이 실제와 가짜 이미지로 혼자 훈련하여 정확해지는 첫번 째 방버을 결정해야한다, 그다음 식별자 모델과 적대적 모델을 순차/반복적으로 훈련한다. 

~~~

    #Load MNIST dataset
(x_train, y_train) , (x_test, y_test) = mnist.load_data()
x_train = x_train.astype('float32') / 255.0
x_train = x_train.reshape((x_train.shape[0],)+ (28, 28, 1))

print(x_train.shape)

#Init network
gan = Gan(x_train)

#some parameters.
epochs = 30
sample_size =10
batch_size = 100
train_per_epoch = x_train.shape[0] //batch_size

for epoch in range(0, epochs):
  print("Epoch:" , epoch+1)

  total_d_loss = 0.0
  total_a_loss = 0.0

  for batch in range(0, train_per_epoch):
    d_loss , a_loss, imgs = gan.train(batch_size)
    total_d_loss += d_loss
    total_a_loss += a_loss
~~~
mnist데이터를 다운 받은후 이를 normalization화한 후 Gan에 X_train데이터를 집어넣어 준비를 끝낸다.  
이후 Gan에 x_train 데이터를 넣어 반복학습을 시킨다.

![noise](https://user-images.githubusercontent.com/65720894/89091418-8d02da00-d3e4-11ea-8779-c5621a60bb7c.PNG)

학습되지 않은 노이즈 데이터이다.


![step300](https://user-images.githubusercontent.com/65720894/89091420-8e340700-d3e4-11ea-9f2c-90310c655326.PNG)

![step500](https://user-images.githubusercontent.com/65720894/89091421-8ecc9d80-d3e4-11ea-8e3e-40c9de5d5ec9.PNG)


![step800](https://user-images.githubusercontent.com/65720894/89091422-8f653400-d3e4-11ea-86dc-d42a063e6fef.PNG)

학습을 하는 도중 갑자기 컴퓨터가 꺼지는 상황(사지방 컴퓨터라 빈약함)이 발생해 약 10시간동안 학습되고있던 데이터가 내곁을 떠나갔다...  
대충 손글씨 처럼 보이지 않는가? 악필이라고 생각하자. 다음에는 이 코드를 변경하여 새로운 이미지를 만들어볼것이다.

전체코드는 코랩에 올려두겠다.  
[MNIST-DCGAN](https://colab.research.google.com/drive/1sCmiGfVZZKAJxN4M4Ti5sBhvBPpWAh5S#scrollTo=TZHd6Wwnj27p)


    
    

