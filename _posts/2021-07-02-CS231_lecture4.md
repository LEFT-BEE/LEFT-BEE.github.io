---
title : "cs231 lecture4"
categories : "cs231"
---

## Lecture 4 : Backpropagation and Neural Networks

4강 내용은 Backpropagation과 Nerueal Networks이다 거의 대부분은 복잡한 함수를 computational graph로 바꾸고 gradient를 어떻게
계산할지에 대한 강의이다.

![image](https://user-images.githubusercontent.com/65720894/124215541-2a074f00-db2f-11eb-9268-34c1ea0337fd.png)

일단 gradient descent를 하기 위해서 매 반복에서 Gradient를 구해야 한다 gradient를 구하는 방법에는 2가지 가 있는데 Numerical gradient나 Anal
ytic gradient를 구하면 되는것이다 이때 수치해를 통해 구하게 된다면 좋지 않은 성능을 보이며 느리지만 사용하기 편하다? 라는 장점이 있다
이에 반해 모든면에서 해석적 방법이 더욱 뛰어나기 때문에 보통 해석해로 구하게된다 ( 수치해로 구하는 것은 해석해로 미분을 하지 못할 경우)

---------------------------

![image](https://user-images.githubusercontent.com/65720894/124227657-013e8400-db46-11eb-82d3-561f0cff90bb.png)

위 방식이 computational graph를 작성하는 방법이다 파라미터가 x,y,z일때 먼저 x+y를 수행한 후에 z를 곱함으로써 복잡한 함수 z를 계산할 수 있게 되는 것이다


![image](https://user-images.githubusercontent.com/65720894/124228247-e28cbd00-db46-11eb-9e2d-6d9ad00b2885.png)


computational graph만 작성한다면 우린 여기서 x,y,z에 대한 f의 변화량을 쉽계 계산 할 수 있다. f에 대한 f의 변화량을 1이라고 두었을떄 x에 대한 q의 변화량을 부하는 것이다.

![image](https://user-images.githubusercontent.com/65720894/124228603-6050c880-db47-11eb-93e9-be3fe83e01f8.png)

위 그래프에서 q=x+y이므로 dq/dx=1 이다. 따라서 dq/dx = 1이고 df/dq=z 이다. z = -4이므로 이둘을 곱한 df/dx는 -4이다.

![image](https://user-images.githubusercontent.com/65720894/124228678-7e1e2d80-db47-11eb-8aa5-8d37205c8f07.png)

이러한 과정을 좀더 일반화 할시 뒤에서 오는 gradients에 gradient를 곱하는 것으로 해석할 수 있다. 

![image](https://user-images.githubusercontent.com/65720894/124228752-99893880-db47-11eb-9b06-64d0d31ef910.png)

위 함수 f의 형태는 logistic sigmoid함수에서 linear combination을 합성한 함수이다. 이것을 computation graph로 만들었고 이렇게 되면 w에 대한
loss를 더 쉽게 구할 수 있는데 그 이유는 f자체를 미분하는 것은 어려운 일이지만 이렇게 작은 단위로 쪼개어 오차역전볍을 통해 미분하게 된다면
훨씬 쉽고 간단하게 그 값을 구할 수 있기 때문이다.

------

![image](https://user-images.githubusercontent.com/65720894/124229522-b6723b80-db48-11eb-823d-78a654c24903.png)

그리고 각 gate들을 gradient관점에서 본다면 distributor , router , switcher로 생각 할 수 있다. 예를들어 add는 미분해보면 x방향이든 y방향으로든
값이 1이다 그리고 합성 함수 꼴로 나타 냈을 때 뒤에서 오는 gradient를 앞으로 그대로 전달하는 꼴이 된다 따라서 distributor로 해석할 수 있는 것이다. 

![image](https://user-images.githubusercontent.com/65720894/124229768-15d04b80-db49-11eb-964e-dcc882afb5ec.png)

local gradient를 구할때 jacobuan matrix로 각 행or열이 derivative vector가 되도록 만들 수 있다 하지만, 이런 jacobian을 사용했을때 문제가 
발생한다.

![image](https://user-images.githubusercontent.com/65720894/124229885-3ac4be80-db49-11eb-9ce4-7c42afab3e47.png)

이러한 elementwise함수에서는 야코비안 행렬의 크기가 variable x variable만큼 나온다 gpu에서 100개의 미니배치를 한번에 처리할때 너무나도 큰 값을
처리하게 되는 상황이 올 수 도 있어 이경우 matrix에서 diagonal(대각선)만 취해서 사용한다.

![image](https://user-images.githubusercontent.com/65720894/124230054-7d869680-db49-11eb-9f6c-0ca9a01d3946.png)


벡터나 행렬연산에서 computational graph는 조금 까다롭다 a = w.dot(x) 이라고 할때 w의 행에 영향을 미치는 것은 x의 열이다 즉 a_(x,y)
를 구하기 위해 w의 x번째행 x의 y번째 열을 곱해서 계산되는데 w의 i번째 행 , j번째 열에 영향을 미치는 x의 factor는 x의 y번째 열 , j번째 행이다 
따라서 위와 같이 계산이 되긴한다.

![image](https://user-images.githubusercontent.com/65720894/124231026-b7a46800-db4a-11eb-8804-f5f8e0a4f265.png)

즉 이러한 공식이 생긴다 




