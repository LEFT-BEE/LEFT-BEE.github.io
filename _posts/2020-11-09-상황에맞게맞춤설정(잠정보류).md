---
title : "상황에 맞게 맞춤설정"
---

지도 학습을 할떄 fit()을 호출하여 학습가능 하다. 이떄 자신만의 training loop를 처음부터 작성해야 할때 `gradientTape`를 사용하여 모든 세부 사항을 제어 할 수 있다.
하지만 독자적인 학습 알고리즘을 사용하면서 fit()의 편리함을 유지하고 싶을떄는 어떻게 해야할까? ex) callbacks , built=in distribution등

Keras의 핵심 원칙은 복잡성을 점진적으로 공개하는 것이다 높은 수준의 편의성을 유지하면서 작은 세부 사항을 더 잘 제어 할 수 있어야 한다. fit()을 사용자 정의해야하는 경우
Model 클래스의 학습 단계 함수를 재정의 해야한다..라고 하는데 예시를 보면서 이해해 보도록하자


### 첫 번째 간단한 예

* keras.Model 이라는 subclases들을 생성한다 

* train(self , data)라는 메소드를 재정의한다

* dictionary maaping metrics names를 반환한다

data라는 것은 우리가 fit안에 training data로 넣어줄 것이다 

* 만약 data가 Numpy arrays일시 fit(x ,y ,...)을 호출함으로써 그리고 data가 tuple의 형태를 가진다 

* 만약 data가 tf.data.Data의 형태일시 fit(dataset , ...)을 호출함으로써 

traing_step의 본문에서는 우리는 주기적인 학습 update를 실행한다 이미 나에게 친숙한 형태로 말이다 중요한 것은 loss를 self.compiled_loss로 계산한다는 점이다
이는 loss function을 감싸 compile()로 넘긴다 

유사하게 self.compiled_metrics.update_state(y , y_pred) 를 metrics를 업데이트 하기위해 호출하는데 metrics는 compile()로 넘겨진다.

```
class CustomModel(keras.Model):
    def train_step(self, data):
        # Unpack the data. Its structure depends on your model and
        # on what you pass to `fit()`.
        x, y = data

        with tf.GradientTape() as tape:
            y_pred = self(x, training=True)  # Forward pass
            # Compute the loss value
            # (the loss function is configured in `compile()`)
            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)

        # Compute gradients
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        # Update metrics (includes the metric that tracks the loss)
        self.compiled_metrics.update_state(y, y_pred)
        # Return a dict mapping metric names to current value
        return {m.name: m.result() for m in self.metrics}
        

```

...(중략)

end to end 예제
솔직히 이번단원 전체가 뭘하고 싶은지 모르곘지만 일단 마무리예제를 살펴보도록 하자
```
from tensorflow.keras import layers

# Create the discriminator
discriminator = keras.Sequential(
    [
        keras.Input(shape=(28, 28, 1)),
        layers.Conv2D(64, (3, 3), strides=(2, 2), padding="same"),
        layers.LeakyReLU(alpha=0.2),
        layers.Conv2D(128, (3, 3), strides=(2, 2), padding="same"),
        layers.LeakyReLU(alpha=0.2),
        layers.GlobalMaxPooling2D(),
        layers.Dense(1),
    ],
    name="discriminator",
)

# Create the generator
latent_dim = 128
generator = keras.Sequential(
    [
        keras.Input(shape=(latent_dim,)),
        # We want to generate 128 coefficients to reshape into a 7x7x128 map
        layers.Dense(7 * 7 * 128),
        layers.LeakyReLU(alpha=0.2),
        layers.Reshape((7, 7, 128)),
        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding="same"),
        layers.LeakyReLU(alpha=0.2),
        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding="same"),
        layers.LeakyReLU(alpha=0.2),
        layers.Conv2D(1, (7, 7), padding="same", activation="sigmoid"),
    ],
    name="generator",
)
```
내가 맨날 만지던 Gan모델이다 mnist데이터를 가지며 generator와 discriminator의 모델이다
다음은 기능이 완전한 GAN 클래스로 자체 서명을 사용하도록 compile()을 재정의 하고 train_step 17줄에 전체 GAN알고리즘을 구현한다

```
class GAN(keras.Model):
    def __init__(self, discriminator, generator, latent_dim):
        super(GAN, self).__init__()
        self.discriminator = discriminator
        self.generator = generator
        self.latent_dim = latent_dim

    def compile(self, d_optimizer, g_optimizer, loss_fn):
        super(GAN, self).compile()
        self.d_optimizer = d_optimizer
        self.g_optimizer = g_optimizer
        self.loss_fn = loss_fn

    def train_step(self, real_images):
        if isinstance(real_images, tuple):
            real_images = real_images[0]
        # Sample random points in the latent space
        batch_size = tf.shape(real_images)[0]
        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))

        # Decode them to fake images
        generated_images = self.generator(random_latent_vectors)

        # Combine them with real images
        combined_images = tf.concat([generated_images, real_images], axis=0)

        # Assemble labels discriminating real from fake images
        labels = tf.concat(
            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0
        )
        # Add random noise to the labels - important trick!
        labels += 0.05 * tf.random.uniform(tf.shape(labels))

        # Train the discriminator
        with tf.GradientTape() as tape:
            predictions = self.discriminator(combined_images)
            d_loss = self.loss_fn(labels, predictions)
        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)
        self.d_optimizer.apply_gradients(
            zip(grads, self.discriminator.trainable_weights)
        )

        # Sample random points in the latent space
        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))

        # Assemble labels that say "all real images"
        misleading_labels = tf.zeros((batch_size, 1))

        # Train the generator (note that we should *not* update the weights
        # of the discriminator)!
        with tf.GradientTape() as tape:
            predictions = self.discriminator(self.generator(random_latent_vectors))
            g_loss = self.loss_fn(misleading_labels, predictions)
        grads = tape.gradient(g_loss, self.generator.trainable_weights)
        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))
        return {"d_loss": d_loss, "g_loss": g_loss}

```
아니 이거 왜 하는거임 ㄹㅇ?




# 위 예제에서 나온 Gradient tape에대하여 

### 자동미분과 gradient tape

tensroflow 는 자동미분(주어진 입력 변수에 대한 연산의 그래디언트를 계산하는것)을 위한 tf.GradientTape API를 제공한다 tf.GradientTape는 
context안에서 실행된 모든 연산을 테이프에 기록한다 그다음 tensorflow는 recerse mode differentioation을 사용해 테이프에 기록된 연산의 그래디언트를 계산한다.

예를 들면
```
x = tf.Variable(3.0)

with tf.GradientTape() as tape:
  y = x**2
````
타겟의 기울기를 계산하기 위해  GradientTape.gradient(target , source) 를 통해 operation을 record 하였다 
```
# dy = 2x * dx
dy_dx = tape.gradient(y, x)
dy_dx.numpy()

결과 
6.0
```
위의 예제는 scalars이지만 tf.GradientTpae는 어떠한 tensorf에서도 잘 작동된다 

```
w = tf.Variable(tf.random.normal((3, 2)), name='w')
b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')
x = [[1., 2., 3.]]

with tf.GradientTape(persistent=True) as tape:
  y = x @ w + b
  loss = tf.reduce_mean(y**2)
````
위와 같은 예제는 y의 gradient를 계산하기위해 두개의 변수가 필요하다 tape는 변수가 전달되는 과정이 유연하다 
```
[dl_dw, dl_db] = tape.gradient(loss, [w, b])
```
각 그래디언트는 source의 모양을 가진다
```
print(w.shape)
print(dl_dw.shape)

(3, 2)
(3, 2)
```
여기 gradient 계산값이 있는데 이번에는 dictionary of variable의 형태로 전달한다
```
my_vars = {
    'w': tf.Variable(tf.random.normal((3, 2)), name='w'),
    'b': tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')
}

grad = tape.gradient(loss, my_vars)
grad['b']
```
### 모델에 대한 gradient 








