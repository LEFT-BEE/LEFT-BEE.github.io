---
title: "Conditional GAN 예제코드(건물을 만들어보자 with TensorFlow examplecode"
categories: GAN
use_math: true
comments: true
---


## Tensorflow example code

```
import tensorflow as tf

import os
import time
from matplotlib import pyplot as plt
from IPython import display

#Load the dataset

_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'

path_to_zip = tf.keras.utils.get_file('facades.tar.gz',
                                      origin=_URL,
                                      extract=True)
#256 x 256 size
PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')
```

기본적인 세팅과 학습에 필요한 데이터를 가져온다.

```
BUFFER_SIZE = 400#의문
BATCH_SIZE = 1
IMG_WIDTH = 256
IMG_HEIGHT = 256

def load(image_file):
  image = tf.io.read_file(image_file)
  image = tf.image.decode_jpeg(image)# 이미지파일을 잘손질

  w = tf.shape(image)[1] #w = tf.Tensor(512, shape=(), dtype=int32) 의 형태이다
  w = w// 2 # 256으로 변환
   #// 나누기 연산후 소수점 이하의 수를 버리고, 정수부분의 수만 구함
  real_image = image[:, :w, :]#앞에있는게 real데이터
  input_image = image[: ,w:, :] #데이터자체가 반으로 나뉘어진것일것  아 걍 그렇다고 하자 데이터가

  input_image = tf.cast(input_image, tf.float32)
  real_image = tf.cast(real_image, tf.float32)

  return input_image, real_image #input image = 내가가진 데이터 , real_data =  워너비 실제데이터
  ```
  
BUFFER_SIZE는 무슨의미인지 아직 불명이다..   
load()함수는 이미지 파일을 입력받아 데이터를 반으로 나누어 real_data와 input_data로 나눈다 
  
  ```
  inp,re = load(PATH+'train/100.jpg')
# casting to int for matplotlib to show the image
plt.figure()
plt.imshow(inp/255.0)#값을 바꾸면서 본 결과로 밝기를 조절할수 있다 즉 0으로 갈수록 어두워지며 1로 갈수록 밝아진다
plt.figure()
plt.imshow(re/255.0)
```

데이터를 확인해 보도록 하자.

![2](https://user-images.githubusercontent.com/65720894/89540699-5e4b9000-d838-11ea-95df-9bd993e6d629.PNG)

위가 input_image 아래가 real_image이다. 255.0으로 나눠주는 것은 각 픽셀의 밝기가 0~1부터의 밝기로 이루어져서이다.

```
def resize(input_iamge , real_image, height, width):
  input_image = tf.image.resize(input_image, [height, width],
                              method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
  real_image = tf.image.resize(real_image, [height, width],
                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
  
  return input_image, real_image #학습에 알맞게 resize해준다
#위 함수는 지터링을 위해 더 큰 높이(286pix)와 너비(286pix)로 이미지 크기를 조정한다. 
```

```
def random_crop(input_image, real_image):
  stacked_image = tf.stack([input_image, real_image], axis=0)#stack 함수는  행렬를 합친다 즉 두 데이터를 합침
  cropped_image = tf.image.random_crop(
      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])# 마지막은 체널이고 첫번쨰는?
  
# tf.random_crop 함수는 첫 번째 파라미터로 받은 텐서타입의 이미지들을 
#두 번째 파라미터로 받은 크기로 무작위로 잘라 첫 번째 받은 파라미터와 같은 rank의 텐서 형태로 돌려준다.

  return cropped_image[0], cropped_image[1]
#256pix인 대상크기로 무작위 자르기를 한다. - 이를jitter라고 하는데 논문에서 소개하고있으니 해보도록 하자
```

```
 # normalizing the images to [-1, 1]

def normalize(input_image, real_image):
  input_image = (input_image / 127.5) - 1
  real_image = (real_image / 127.5) - 1

  return input_image, real_image
  ```
  
  ```
  def random_jitter(input_image, real_image):
  # resizing to 286 x 286 x 3
  input_image, real_image = resize(input_image, real_image, 286, 286)#286으로 재조정

  # randomly cropping to 256 x 256 x 3
  input_image, real_image = random_crop(input_image, real_image)#256사이즈로 랜덤하게 자른다. 아마 학습
  #률을 높이기위해(분포값을 늘리기위해?)

  if tf.random.uniform(()) > 0.5:#균일 한 분포로부터의 출력값을 랜덤.
    # random mirroring
    input_image = tf.image.flip_left_right(input_image)
    real_image = tf.image.flip_left_right(real_image)#이미지 d

  return input_image, real_image
  ```
  
  코드가 길어졌는데 사실 내용은 별거 없다. jitter와 mirriring을 해주었는데 그 이유는 CGAN 논문에서 학습률에 도움을 준다고 하여 집어넣었다. 예상하는 바로는 각 이미지 데이터의 분포를 항상 랜덤하게 만들어 학습이 더 효과적으로 이루어질 수 있게 하는것이라 생각한다.
  ![1](https://user-images.githubusercontent.com/65720894/89540695-5d1a6300-d838-11ea-804f-87aec51fcd59.PNG)
  
  
  위는 눈문의 내용중을 캡쳐하였다.
  
  ![3](https://user-images.githubusercontent.com/65720894/89541357-3ad51500-d839-11ea-9775-64cf239123f5.PNG)
  하지만 현재 위의 오류가 뜨면서 나의 진행을 방해하고있다. 문제의 해결법을 찾았는데... 단순한 오타였다 하하
  
  ```
   plt.figure(figsize=(6, 6))
for i in range(4):
  rj_inp, rj_re = random_jitter(inp, re)
  plt.subplot(2, 2, i+1)
  plt.imshow(rj_inp/255.0)
  plt.axis('off')
plt.show()
```
![캡처](https://user-images.githubusercontent.com/65720894/89653837-ad5df780-d902-11ea-91b1-b6d2e1842cc1.PNG)

이미지 데이터로 시험해보니 잘 작동하는것 같다.

```
 def load_image_train(image_file):
  input_image, real_image = load(image_file)
  input_image, real_image = normalize(input_image, real_image)

  return input_image, real_image
  #image train 용 데이터  호출
  
  def load_image_test(image_file):
  input_image, real_image = load(image_file)
  input_image, real_image = resize(input_image, real_image,
                                   IMG_HEIGHT, IMG_WIDTH)
  input_image, real_image = normalize(input_image, real_image)

  return input_image, real_image #test용 이미지 호출 애는 왜 resize를 할까요?
  ```
  
  이미지의 test_data와 train_data는 다른방식으로 로드된다. 
  
  ```
  #입력 파이프라인
train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')
train_dataset = train_dataset.map(load_image_train,
                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)#살짝 이해안감
train_dataset = train_dataset.shuffle(BUFFER_SIZE)#섞어준다
train_dataset = train_dataset.batch(BATCH_SIZE)#batch size로 배열을 나눠준다 reminder = true 설정은 남는 배열값을
#무시하고 만든다.

test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')
test_dataset = test_dataset.map(load_image_test)
test_dataset = test_dataset.batch(BATCH_SIZE)#test용 데이터셋을 만들어준다
```

각각의 데이터 셋을 만드는 과정인데 데이터를 이후 사용할 수 있게 잘 다듬어 준다.

```
 OUTPUT_CHANNELS = 3
  
   def downsample(filters, size, apply_batchnorm=True):
  initializer = tf.random_normal_initializer(0., 0.02)#0이 평균 0.02가 표준편차인 텐서 생성

  result = tf.keras.Sequential()
  result.add(
      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',
                             kernel_initializer=initializer, use_bias=False))#input image가 없는건가 이후 
                             #tf.keras.latyers.Input(shape) 함수에서 입력값을 넣어준다.
                             #kenel_initalizer는 정규 분포와 텐서를 생성한다.
                             #근데 왜 생성해서 매개변수에 넣어주는걸까..?
                             #답은 가중치 초기화에 있었다.

  if apply_batchnorm:
    result.add(tf.keras.layers.BatchNormalization())

  result.add(tf.keras.layers.LeakyReLU())

  return result
  
 #인코더의 각블록은 (Conv - Batchnorm - Leaky ReLU)이다.
 #블록은 즉 result 하나의 layer라 할수 있지 않을까?
 ```
 
 downsample 데이터배열을 합성곱(convolution)하여 feature map을 만든다 이때 하나의 layer에는 conv2d - Batchnormalization - Leaky Relu 순으로 구성되어있다.
 
 ```
down_model = downsample(3, 4)
down_result = down_model(tf.expand_dims(inp, 0))
print (down_result.shape)
```

`result : (1, 128, 128, 3)`

```
def upsample(filters, size, apply_dropout=False):
  initializer = tf.random_normal_initializer(0., 0.02)

  result = tf.keras.Sequential()
  result.add(
    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,
                                    padding='same',#패딩그대로 
                                    kernel_initializer=initializer,
                                    use_bias=False))
  

  result.add(tf.keras.layers.BatchNormalization())

  if apply_dropout:
      result.add(tf.keras.layers.Dropout(0.5))

  result.add(tf.keras.layers.ReLU())

  return result
 #디코더의 블록은 (Transposed Conv - Batchnorm - Dropout) - ReLU 으로 구성되있다.
 ```
 
 upsample 은 Transepose_Conv2d로 구현 가능하며 이와 관련된 개념은 이후 차차 알아볼것이다. 여기서는 이미지 데이터를 확장 시키므로 Conv2d와 반대의 입장이라 이해하면 쉽다. 각각의 배열은 (Transposed Conv - Batchnorm - Dropout)-ReLU로 구성된다. RELU는 처음 3 개의 블록에 적용됨
 
 ```
 def Generator():
  inputs = tf.keras.layers.Input(shape=[256,256,3]) #keras 모델에 input데이터 값을 입력해준다. 

  down_stack = [
    downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64) 첫번쨰만 배치정규화를 안했다.
    downsample(128, 4), # (bs, 64, 64, 128)
    downsample(256, 4), # (bs, 32, 32, 256)
    downsample(512, 4), # (bs, 16, 16, 512)
    downsample(512, 4), # (bs, 8, 8, 512)
    downsample(512, 4), # (bs, 4, 4, 512)
    downsample(512, 4), # (bs, 2, 2, 512)
    downsample(512, 4), # (bs, 1, 1, 512)
  ]

  up_stack = [
    upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)
    upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)
    upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)
    upsample(512, 4), # (bs, 16, 16, 1024)
    upsample(256, 4), # (bs, 32, 32, 512)
    upsample(128, 4), # (bs, 64, 64, 256)
    upsample(64, 4), # (bs, 128, 128, 128)
  ]#이 개념은 나중에 다시 세세히 알아보자 - 위 데이터 shape값은 downsample의 데이터값이 concatenate된 값으로 
  #실제의 두배의 데이터의 차원이 있다.

  initializer = tf.random_normal_initializer(0., 0.02)
  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,
                                         strides=2,
                                         padding='same',
                                         kernel_initializer=initializer,
                                         activation='tanh') # (bs, 256, 256, 3)

  x = inputs

  # Downsampling through the model
  skips = []
  for down in down_stack:
    x = down(x)
    skips.append(x)

  skips = reversed(skips[:-1])# 파이썬에서 배열의 -1값은 맨 마지막 데이터이므로 마지막 데이터를 제외한 모든 값이다.

  # Upsampling and establishing the skip connections
  for up, skip in zip(up_stack, skips):
    x = up(x)
    x = tf.keras.layers.Concatenate()([x, skip])# 위의 데이터를 합치는데 이게바로 U-NET방식이다 학습률을 높여주지

  x = last(x)

  return tf.keras.Model(inputs=inputs, outputs=x)
  ```
  생성자(Generator)를 구현하였고 이는 이 그림을 통해 이해하면 쉽다
  
  ![generator](https://camo.githubusercontent.com/6bae649298f16ed7645a607615c9e97470cd873f/68747470733a2f2f7461656f682d6b696d2e6769746875622e696f2f696d672f636f6465312e504e47)</center>
  
downsample로 256x256x3의 이미지 데이터를 1x1x512 feature변환 시킨후 이를 upsample시켜 원래의 이미지 크기로 다시 변환한다.  
이때 upsample에는 ` x = tf.keras.layers.Concatenate()([x, skip])` 이전에 downsample했던 이미지 데이터가 역배치로 함께 들어가게된다. 또한 upsample의 마지막은 따로 Cnnv2d를 설정해준다.


  
  
  
  
 
 



  
  
  

