---
layout: post
title: "[CS231N] lecture2 review"
categories: summary
tags: summary cs231n
comments: true
---

본 글 내용에서는 cs231 스터디내용을 간단하게 요약하고 주로 새로 알게된 사실을 서술한다.

## About Image Classfication

우리는 아래와 같은 고양이를 한 눈에 보자마자 고양이라고 인식 할 수 있다 하지만 컴퓨터에게 있어 이는 매우 어려운 문제가 된다.
컴퓨터에게 있어 이미지란 데이터의 집합일 뿐이고 이를 판독하여 무엇을 나타내는지는 알 수 가 없다. 또한 고양이와 같은 생물은 그 생김새가 다양하며
매우 유연한 동물이기에 특정 형태를 띄고 있다고 하기도 어렵다

![image](https://user-images.githubusercontent.com/65720894/122865480-6ed9fb80-d361-11eb-96f2-3b464e4cff92.png)


컴퓨터가 이러한 이미지를 구분하는 과정은 train과정과 predict과정이 있다 train과정에서 컴퓨터는 각각의 이미지가 무엇을 나타내는지 학습하게 되고 
predict과정에서는 이를 판독하게 된다. 그렇다면 train을 하는 방법에는 어떤 것들이 있을까?


### Nearest Neighbor

이미지 데이터를 기억한다음 제일 비슷한 이미지와의 차이를 통해 차이가 제일 작은 레이블을 정답으로 판독한다. 이때 차이를 구하는 방법이 두 가지가 있는데 하나는 L1 distance로 단순하게 이미지 픽셀값의 차이의 절대값을 모두 합친 값을 의미한다 L1 distance의 수식은 아래와 같다.

![image](https://user-images.githubusercontent.com/65720894/122865958-2838d100-d362-11eb-8468-ccd4e438ba2f.png)


분류되는 과정을 시각적으로 나타내게 되면 다음과 같은데 위의 K-Nearest Neibors는 L1 distance를 통해 주변의 k개의 객체들과의 비교를 하는
알고리즘을 의미한다 그림이 나타내고 있듯이 K가 높아질 수록 분류됨에 있어 세밀해짐을 알 수 있다 cf)하얀색으로 나뉘어지지 않은 곳은 knn알고리즘의 한계로 인해 나뉘어지지 못한다.

![image](https://user-images.githubusercontent.com/65720894/122866072-58806f80-d362-11eb-8375-c6d10697167d.png)

데이터의 종류에 따라 L1 distance를 사용하기도 하고 L2 distance를 사용하기도 한다 일반적으로 이미지와 같은 연속적인 스펙트럼을 가지는 데이터는 L2 distance를 사용하고 label값이 나뉘어 떨어지는 느낌이면 L1 disctance를 사용하는 것이 성능에 도움이 된다.


---------------------

#### Hyperparameters

knn알고리즘에 있어 K 나 distance종류를 Hyperparameter라고 한다 이는 알고리즘을 설계하는데 있어 구성요소들을 의미하고 성능에 큰 영향을 미친다. 이를 구하는 방법은 주로 휴리스틱한 방법으로 자주 구하게된다. 



#### setting hyperparameter

하이퍼 파라미터를 구하는 방법은 휴리스틱한 방법으로 구하게된다 여러번의 반복을 통해 제일 좋은 값을 찾아 내는 것을 의미하는데 이때 학습하는 과정에서 데이터를 test train validation으로 나누는 것이 제일 효율적이다. train set은 데이터를 훈련시키는 데이터이고 test는 훈련된 모델을
평가하는 data이다 validation data는 주로 교차되어서 train데이터로 학습된 모델을 검증하는데 에를들어 데이터를 5개로 나누어 하나하나 validation 과정을 거쳐 성능이 제일 좋은 모델을 test해보는 것이다



--------------

#### knn in images never used

knn알고리즘은 실제로 성능이 꽤 좋지 못하다 픽셀값의 차의 총합이 이미지를 구분하는데 있어 객관적인 지표가 되지 못하기 떄문인데 아래 그림과 같이 전혀다른 이미지가 있음에도 모두 값은 distance를 가진다 이는 이미지를 구분하는데 있어 효율적이지 못하다는 증거이다.

![image](https://user-images.githubusercontent.com/65720894/122868035-5d92ee00-d365-11eb-87a9-4027e4d49b41.png)


### parametic Approach

따라서 knn보다 효율적인 방안을 소개한다 아래 예시 처럼 이미지를 주게되면 어떠한 함수를 통해 이미지를 구분하게 하는 것이다 이때 이미지는 파라미터을 통해 특징이 추출된다.

![image](https://user-images.githubusercontent.com/65720894/122868661-3852af80-d366-11eb-9266-6b7c80726ab8.png)

반복을 통해 파라미터 W는 이미지의 특징을 더욱 잘 추출하게 되고 이를 학습시킨다라고 정의한다 아래는 4pixel의 이미지를 판독해 3의 레이블로 점수를 매긴 예시이다.

![image](https://user-images.githubusercontent.com/65720894/122868934-7f40a500-d366-11eb-861a-079b81758517.png)


### inerpreting a Linear Classifer

그렇다면 이러한 과정을 공간상에 나타낸다면 아마 아래와 같은 그림일 것이다. 아래 이미지에서 공간을 구분하고 있는 선들이 W이다 즉 이미지를 벡터화 시켜 얼마나 W 벡터와 근사한지를 수치로 나타내기위해 WX 즉 내적을 통해 방향으로 결정한다. 따라서 학습을 할수록 w는 이미지와 비슷한 형태를 지니게 된다.  

### Hard case for a linear classifer

하지만 wx+b와 같은 선형적 함수로 해결하지 못하는 문제 들이 많다 아래의 그림과 같이 간단한 형태의 패턴을 학습하지 못하는데 이를 보완할 필요가 있다고 설명한다

![image](https://user-images.githubusercontent.com/65720894/122869327-10178080-d367-11eb-8847-acc8a3625ea1.png)















