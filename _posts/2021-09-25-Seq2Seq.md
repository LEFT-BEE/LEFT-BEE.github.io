---
layout: post
title: "[Model Review] Seq2Seq Reivew"
categories: deeplearning
tags: paper model
comments: true
---

##seq2seq란?

기존 Deep Neural Network(DNN) 은 speech recognition, visual object recognition 등 다양한 분야에서 뛰어난 성능을 제공한다. 하지만, DNN은 크기(차원)이 고정된 입력에 대해서만 데이터를 처리할 수 있다는 단점이 있다. 대부분의 경우 입력의 크기를 미리 알고 있을 수 없기 때문에 DNN의 단점은 더 치명적일 수 있다. 입력의 크기를 알 수 없는 sequential problem 들은 speech recognition, machine translation 등이 있다. 

DNN의 이러한 단점을 보완하기 위해서 이번 논문에서는 Long Short-Term Memory (LSTM) 을 사용한다. 총 두개의 LSTM을 사용하는데, 첫 번째 LSTM을 입력을 순차적으로 받아서 마지막에 큰 차원의 vector representation을 생성한다. 첫 번째 LSTM에서 생성한 vector representation을 두 번째 LSTM의 initial hidden state로 사용해서 decoding 을 수행한다. 이렇게 하는 경우, decoder LSTM에서 생성하는 결과는 입력 sequence에 의존적으로 생성된다.  

![image](https://user-images.githubusercontent.com/65720894/134767034-24f28cff-ab1f-4ea0-8904-03e7e5cc392a.png)


[그림-1]을 보면 입력으로 "ABC"가 첫 번째 LSTM에 입력되고, 해당 입력에 대한 결과(번역)인 "WXYZ" 가 두 번째 LSTM을 통해서 제공된다. 각 문장의 끝은 
<
E
O
S
>
 라는 토큰으로 구분한다. 그러므로, 사실상 첫 번째 LSTM에는 A, B, C, <EOS> 가 입력으로 제공되는 것이고, 두 번째 LSTM은 W, X, Y, Z, <EOS> 를 출력으로 제공하게 된다. 

LSTM은 long sequence 에 대해서 취약하는 연구들이 있었지만, 이번 논문에서 제안하는 모델에서는 긴 문장에 대해서도 좋은 성능을 제공할 수 있었다. 입력을 뒤집어서 거꾸로 입력하는 방식을 통해서 긴 문장에 대해서도 높은 성능을 유지할 수 있었다. 

LSTM을 이용해서 가변 길이의 입력에 대해서 유연하게 처리할 수 있었다. LSTM의 큰 장점 중 하나는, 이러한 가변 길이으 입력에 대해서 고정된 길이의 vector representation을 생성할 수 있다는 것이다. Translation을 수행하기 위해서 어느정도의 paraphrasing 을 수행하게 되는데, vector representation은 입력 문장의 의미를 paraphrased 한 vector representation을 생성한다고 볼 수 있다. 
  유사한 의미의 문장에 대해서 유사한 vector representation을 생성하게 된다. 
  
  
  ## 모델의 구조 
  
  기존의 Recurrent Neural Network (RNN) 은 입력 
(
x
1
,
x
2
,
.
.
.
,
x
T
)
 에 대한 출력 
(
y
1
,
y
2
,
.
.
.
,
y
T
)
 을 제공한다. Hidden state와 output은 아래식과 같이 계산하게 된다.
  하지만, 입력 길이와 출력 길이의 연관성을 알 수 없을 때 RNN을 적용하기엔 제한이 있다. 
  
  ![image](https://user-images.githubusercontent.com/65720894/134767051-f862c6ca-5b0d-46ea-8ee0-15176ec39646.png)
  
  이러한 문제를 해결하기 위한 가장 단순한 방식은 첫 번째 RNN을 이용해서 입력에 대한 고정된 차원의 vector를 생성하고, 두 번째 RNN에 해당 vector를 입력으로 제공해서 다른 sequence를 결과로 구하는 방법이다. 이 방법에는 조금 더 근본적인 문제가 있는데, 그것은 RNN이 long-term dependency를 잘 학습하지 못한다는 것이다. LSTM은 반면에 이러한 long-term dependency를 RNN에 비해 더 잘 학습할 수 있다. 
LSTM은 입력 
(
x
1
,
x
2
,
.
.
.
,
x
T
)
 와 출력 
(
y
1
,
y
2
,
.
.
.
,
y
T
′
)
 에 대해 conditional probability 
p
(
y
1
,
y
2
,
.
.
.
,
y
T
′
|
x
1
,
x
2
,
.
.
.
,
x
T
)
 를 예측하는 것을 목표로 한다. 입력에 대한 마지막 hidden state를 고정된 차원의 representation 
v
 라고 할 때, 
y
1
,
y
2
,
.
.
.
,
y
T
′
 의 확률은 다음과 같이 정의할 수 있다. 
  ![image](https://user-images.githubusercontent.com/65720894/134767125-faf3f2b3-2985-4495-8dcc-2381e353e0ec.png)
  
  이번 논문에서 제안하는 모델은 다음과 같은 추가적인 구조를 따른다.
  1. 입력과 출력에 대한 서로 다른 2개의 LSTM을 사용한다.
  2.Deep LSTM의 Shallow LSTM 보다 좋은 성능을 제공한다. 그러므로, 총 4개의 층으로 구성된 LSTM을 사용한다.
  3.입력의 순서를 뒤집어서 제공한다. a, b, c, 로 구성된 문장이 있다면, 모델의 입력으로는 c, b, a 로 제공한다. 입력을 뒤집어서 제공하는 방법은 long-term dependency 를 더 잘 학습할 수 있게 한다. 이번 논문에서도 입력을 뒤집는 것이 성능에 긍정적인 영향을 끼치는 이유를 정확하게 설명하지는 못하지만, 어느정도 예측한 가설을 설명한다. 입력 문장을 뒤집어도 source word 와 target word 사이 거리의 평균을 동일하게 유지된다. 하지만, "minimal time lag" 가 크게 줄어들기 때문에 이러한 성능의 향상이 있을 수 있다고 설명한다. 

  
